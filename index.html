<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation">

  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Multimodal Generation, Multimodal Benchmark, Multimodal Learning, Vision and Language Dataset, Dialect Robustness">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>DialectGen</title>
  <!-- Google Tag Manager -->
  <script>(function (w, d, s, l, i) {
    w[l] = w[l] || []; w[l].push({
      'gtm.start':
        new Date().getTime(), event: 'gtm.js'
    }); var f = d.getElementsByTagName(s)[0],
      j = d.createElement(s), dl = l != 'dataLayer' ? '&l=' + l : ''; j.async = true; j.src =
        'https://www.googletagmanager.com/gtm.js?id=' + i + dl; f.parentNode.insertBefore(j, f);
  })(window, document, 'script', 'dataLayer', 'GTM-MFCT45H');</script>
  <!-- End Google Tag Manager -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="icon" href="static/images/icon.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  </head>
  <body>
  <!-- Google Tag Manager (noscript) -->
  <noscript><iframe src="https://www.googletagmanager.com/ns.html?id=GTM-MFCT45H" height="0" width="0"
      style="display:none;visibility:hidden"></iframe></noscript>
  <!-- End Google Tag Manager (noscript) -->

  <nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://dialectgen.github.io/">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://embodied-agent-interface.github.io/">
            Embodied Agent Interface
          </a>
          <a class="navbar-item" href="https://contrastive-visual-data-augmentation.github.io/">
            CoDA
          </a>
          <a class="navbar-item" href="https://arxiv.org/pdf/2310.15066">
            ENVISION
          </a>
        </div>
      </div>
    </div>

  </div>
  </nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title"><img src="static/images/icon.png" width="50" /> DialectGen: Benchmarking and Improving Dialect Robustness in Multimodal Generation</h1>
            <div class="is-size-5 publication-authors">
                <span class="author-block">
                  <a href="https://yu-bryan-zhou.github.io/" target="_blank">Yu Zhou</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://cownowan.github.io/" target="_blank">Sohyun An</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://haikangdeng.github.io/" target="_blank">Haikang Deng</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://wadeyin9712.github.io/" target="_blank">Da Yin</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://clarkipeng.github.io/" target="_blank">Clark Peng</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://web.cs.ucla.edu/~chohsieh/" target="_blank">Cho-Jui Hsieh</a><sup>1</sup>,&emsp;
                </span>
                <br>
                <span class="author-block">
                  <a href="http://web.cs.ucla.edu/~kwchang/" target="_blank">Kai-Wei Chang</a><sup>1</sup>,&emsp;
                </span>
                <span class="author-block">
                  <a href="https://violetpeng.github.io/" target="_blank">Nanyun Peng</a><sup>1</sup>&emsp;
                </span>
                </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>1</sup>UCLA&emsp;
                    </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      
                    </span>
                    <!-- <span class="eql-cntrb"><small><br><sup>*</sup>Equal Contribution</small></span> -->
                  </div>
<!-- 
                  <div class="is-size-5 publication-authors">
                    <span class="author-block">
                      <sup>*</sup>Equal Leadership&emsp;
                      <sup>â€ </sup>Equal Contribution
                    </span>
                  </div> -->

                  <div class="content has-text-centered">
                    <div class="publication-links">
                      <span class="link-block">
                        <a href="https://arxiv.org/" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <span class="link-block">
                      <a href="https://huggingface.co/datasets/uclanlp/" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>HF Dataset</span>
                    </a>
                  </span>
                  </span>

                  <span class="link-block">
                    <a href="https://github.com/DialectGen/DialectGen" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                  </span>
              </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser image-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <!-- <h2 class="title is-3 has-text-centered">DialectGen</h2> -->
      <h2 class="subtitle has-text-justified">
        Multimodal Generative Model Outputs on synonymous prompts that differ only in one synonymous lexical feature from <b><a style="color:#00B0F0;">Standard American English</a></b> (top) or a <b><a style="color:#A02B93;">lower-resource English dialect</a></b> (bottom). </h2>
      <img src="static/images/teaser.png" height="100%"/>
      <!-- <h2 class="subtitle has-text-centered">Example tasks in <span style="font-weight:bold;">BLINK</span>.</h2> -->
      <h2 class="hero-body has-text-centered">
        <!--  -->
      </h2>
    </div>
  </div>
</section>
<!-- End teaser image -->


<!-- Paper abstract -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Contact languages like English exhibit rich regional variations in the form of dialects, which are often used by dialect speakers interacting with generative models. However, can multimodal generative models effectively produce content given dialectal textual input? In this work, we study this question by constructing a new large-scale benchmark spanning six common English dialects. We work with dialect speakers to collect and verify over 4200 unique prompts and evaluate on 17 image and video generative models. Our automatic and human evaluation results show that current state-of-the-art multimodal generative models exhibit 32.26% to 48.17% performance degradation when a single dialect word is used in the prompt. Common mitigation methods such as fine-tuning and prompt rewriting can only improve dialect performance by small margins (< 7%), while potentially incurring significant performance degradation in Standard American English (SAE). To this end, we design a general encoder-based mitigation strategy for multimodal generative models. Our method teaches the model to recognize new dialect features while preserving SAE performance. Experiments on models such as Stable Diffusion 1.5 show that our method is able to simultaneously raise performance on five dialects to be on par with SAE (+34.4%), while incurring near zero cost to SAE performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">DialectGen -- Benchmark</h2>
        <h2 class="content has-text-justified">
          <img src="static/images/dataset.png" height="100%"/>
          <br>
        <ul>
            Paired <b><a style="color:#00B0F0;">SAE</a></b> / <b><a style="color:#A02B93;">Dialect</a></b> data in DialectGen, including Lexeme, Concise Prompt, and Detailed Prompt.
          <br>
        </ul>
        <img src="static/images/main_results.png" height="100%"/> <br>
      </div>
    </div>
  </div>
</section>


<!-- Paper Qualitative -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Mitigation Method</h2>
        <img src="static/images/method.png" width="100%"/>
        <h2 class="content has-text-justified">
          Losses used in our mitigation. Text prompts for Dialect Learning and Polysemy Control come from the DialectGen training set, while image-caption pairs for KL Regularization come from the MSCOCO validation set.
        </h2>
        <img src="static/images/sd15_mitigation_results.png" height="100%"/> <br>
      </div>
    </div>
  </div>
</section>


<!-- Paper Aualitative -->
<section class="section hero">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-3">Qualitative Results</h2>
        <img src="static/images/qualitative.png" height="90%"/>
        <h2 class="content has-text-centered">
          Qualitative Comparison of Mitigation Strategies including generation results using Stable Diffusion 1.5 Base Model, Fine-tuning with Diffusion DPO, and Ours (Dialect Learning + Image KL Regularization + Polysemy Control).
        </h2>
        <h2 class="content has-text-justified">
        </h2>
      </div>
    </div>
  </div>
</section>


<!-- Paper Analysis -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3">Experiment Analysis</h2> <br>
    </div>
    <div class="columns is-centered">
      <div class="column is-five-sixths">
        <h2 class="title is-5">
        1. Why can proprietary models better utilize retrieved images?
        <h2 class="content has-text-justified">
          We conduct an error analysis on an open-source model (LLaVA-Next-Interleave) and a proprietary model (Gemini Pro). As the example illustrated 
          in the Figure, the retrieved images contain two correct examples and three false examples. While 
          Gemini Pro is able to utilize all retrieved images, LLaVA-Next-Interleave leverages bad examples 
          and makes wrong prediction. This example helps explain why do almost all open-source models 
          have lower performance with retrieved knowledge. 
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/error_analysis.png" width="95%"/> </div>
        <br>
        <h2 class="title is-5">2. How much can visual knowledge benefit more than textual knowledge?</h2>
        <h2 class="content has-text-justified">
          We used the Wikipedia corpus as of 2023/07/01 as our text knowledge corpus. To ensure a fair 
            comparison, we employed the same multimodal retriever (CLIP) for retrieving either text or image 
            knowledge. The top-5 ranked documents or images are used for augmenting the input. We selected 
            one open-source (LLaVA-Next-Interleave) and one proprietary (GPT-4-Turbo) LVLM to examine 
            their preference for textual knowledge versus image knowledge on <b>MRAG-BENCH</b>. All the results in the table demonstrate that retrieving visual knowledge is more 
            helpful than retrieving text on <b>MRAG-BENCH</b>. 
        </h2>
        <div class="columns is-centered has-text-centered">
        <img src="static/images/image_v_text.png" width="95%"/> </div>
        <br>
        <h2 class="title is-5">3. How does retriever performance affect LVLMs? </h2>
        <h2 class="content has-text-justified">
          <ul>
            <li>
              As in the left figure, we evaluated LLaVA-Next-Interleave with 4 different multimodal retrievers. When retrievers achieve higher Recall@5 scores (i.e., better retrieved 
              examples), the LVLM's accuracy tends to improve, demonstrating a strong 95% positive correlation. 
              Interestingly, despite similar Recall@5 scores from CLIP and VISTA retrievers, LLaVA-Next-
              Interleave demonstrated a 2.07% gap in overall accuracy. We conjecture that the order of the 
              correctly retrieved examples may also impact the model's final performance. The sensitivity to 
              the order of retrieved examples is a common issue that persists across various models. Although 
              this phenomenon, known as position bias, has been examined in text-based RAG, its impact on visual RAG remains unexplored, presenting a promising direction for future research.
            </li>
          </ul>          
        </h2>
        <h2 class="title is-5">4. How many ground-truth image examples are needed? </h2>
        <h2 class="content has-text-justified">
          <ul>
            <li>
              As in the right figure, we evaluated LLaVA-Next-Interleave using 1, 
              2, 3, 5, 10, 20 GT examples, averaging the results across three random seeds for sampling the 
              GT examples. LLaVA-Next-Interleave saw the greatest improvement of 5.64% with just one GT 
              example. Performance continued to increase steadily, reaching a peak at 10 GT examples, which 
              was 0.29% higher than with 20 GT examples. One possible explanation could be LLaVA-Next-
              Interleave may not able to better leverage visually augmented knowledge in long context scenarios. 
              Moreover, the complexity of questions affects the number of images needed too, one ground-truth 
              example sometimes help the model the most on <b>MRAG-BENCH</b>. We encourage the research on 
              adaptatively deciding the number of necessary images based on the complexity of questions. 
            </li>
          </ul>          
        </h2>

        <div class="columns is-centered has-text-centered">
          <img src="static/images/analysis_new.png" width="95%"/> 
        </div>
      </div>
    </div>
  </div>
</section> -->



<!--BibTex citation -->
  <!-- <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{hu2024mragbench,
          title={MRAG-Bench: Vision-Centric Evaluation for Retrieval-Augmented Multimodal Models},
          author={Hu, Wenbo and Gu, Jia-Chen and Dou, Zi-Yi and Fayyaz, Mohsen and Lu, Pan and Chang, Kai-Wei and Peng, Nanyun},
          journal={arXiv preprint arXiv:2410.08182},
          year={2024}
        }
      </code></pre>
    </div>
</section> -->
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from theÂ <a href="https://nerfies.github.io" target="_blank">Nerfies</a>Â project page.
            
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
